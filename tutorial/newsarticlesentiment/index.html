<!DOCTYPE html>
<html lang="en">
<head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# website: http://ogp.me/ns/website#">
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
    <meta name="description" content="Sentiment Analysis in Python">
    <meta property="og:title" content="Introduction to Identifying News Article Sentiment">
    
    <meta property="og:type" content="article">
    <meta property="article:published_time" content="2018-06-15">
    
    <meta property="og:description" content="Sentiment Analysis in Python">
    <meta property="og:url" content="https://joecoelhosj.github.io/tutorial/newsarticlesentiment/">
    <meta property="og:site_name" content="Big Data Ethics">
    
    <meta name="generator" content="Hugo 0.42.1" />
    <title>Introduction to Identifying News Article Sentiment &middot; Big Data Ethics</title>
    
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    
    <link rel="stylesheet" href="https://joecoelhosj.github.io/css/style.css">
    
    <link href="https://joecoelhosj.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Big Data Ethics" />
    
    
    <link rel="icon" href="https://joecoelhosj.github.io/favicon.ico" />
    

    
    
</head>
<body>

<nav class="navbar navbar-default navbar-fixed-top visible-xs">
	<div class="container-fluid">
		<div class="navbar-header">
			<button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			
				<a class="navbar-brand" href="https://joecoelhosj.github.io/">BIGDATA ETHICS</a>
			
		</div>
		<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
			<ul class="nav navbar-nav">
				
				
					<li><a href="https://joecoelhosj.github.io/about">About</a></li>
				
					<li><a href="https://joecoelhosj.github.io/post/">Blog</a></li>
				
					<li><a href="https://joecoelhosj.github.io/tutorial/">Tutorials</a></li>
				
			</ul>
		</div>
	</div>
</nav>
<div class="container-fluid">
	<div class="row">
		<div id="menu" class="hidden-xs col-sm-4 col-md-3">
	<div id="menu-content" class="vertical-align">
		
			<h1 class="text-center"><a href="https://joecoelhosj.github.io/">BIGDATA ETHICS</a></h1>
		
		
		
			<small class="text-center center-block">Joe Coelho</small>
		
		
		
			<img id="profile-pic" src="https://joecoelhosj.github.io//img/profile.png" alt="My Picture" class="img-circle center-block">
		
		<div id="social" class="text-center">
			
				<a href="https://twitter.com/joebencoelho"><i class="fa fa-twitter fa-2x"></i></a>
			
				<a href="https://www.linkedin.com/in/joecoelhosj/"><i class="fa fa-linkedin fa-2x"></i></a>
			
				<a href="https://plus.google.com/&#43;JosephCoelhoSJ"><i class="fa fa-google-plus fa-2x"></i></a>
			

			<a href="mailto:joseph.coelho@marquette.edu"><i class="fa fa-envelope-o fa-2x"></i></a>
		</div>
		<div id="links" class="text-center">
			
			
				<a href="https://joecoelhosj.github.io/about">About</a>
			
				<a href="https://joecoelhosj.github.io/post/">Blog</a>
			
				<a href="https://joecoelhosj.github.io/tutorial/">Tutorials</a>
			
		</div>
	</div>
</div>

		<div id="content" class="col-xs-12 col-sm-8 col-md-9">
			<div class="row">
				<div id="post" class="col-sm-offset-1 col-sm-10 col-md-10 col-lg-8">

<main>
	<header>
		<h1>Introduction to Identifying News Article Sentiment</h1>
	</header>

	<article>
		

<h3 id="sentiment-analysis">Sentiment Analysis</h3>

<p>Identifying the sentiment of a news article or tweet can be useful in many ways. For example, sentiment of news about stock or the economy might correlate to movements in the stock market. Or sentiment of tweets of an user might correlate to the mood of the user. These in turn can be used for personalizing recommendations or for predicting market sentiment. In this brief tutorial we will look at how news article sentiment can be guaged.</p>

<h3 id="python-packages">Python Packages:</h3>

<pre><code>hello
</code></pre>

<h3 id="getting-a-news-article">Getting a News Article</h3>

<p>We have already seen how tweets can be collected. In this example we will work with news articles. BeautifulSoup and Scrapy are two popular Python packages for scraping news web-sites and getting links to articles. Another popular Python package that helps collect news articles is &lsquo;news-please&rsquo;.</p>

<h2 id="step-1-get-the-news-article-in-this-case-we-are-providing-the-url-for-the-news-article">Step 1: get the news article (in this case we are providing the url for the news article).</h2>

<h3 id="websites-can-be-scraped-for-news-article-links-and-those-can-be-used-for-the-actual-analysis">Websites can be scraped for news article links and those can be used for the actual analysis.</h3>

<h3 id="newsplease-is-a-python-package-that-enables-extracting-news-articles">&ldquo;newsplease&rdquo; is a Python package that enables extracting news articles.</h3>

<pre><code class="language-python">import newsplease
</code></pre>

<pre><code class="language-python">article = newsplease.NewsPlease.from_url('https://www.cnn.com/2018/06/21/us/undocumented-migrant-children-detention-facilities-abuse-invs/index.html')
</code></pre>

<pre><code class="language-python">print(article.title)
</code></pre>

<pre><code>Children allege grave abuse at migrant detention facilities
</code></pre>

<pre><code class="language-python">print(article.text)
</code></pre>

<pre><code>(CNN) A year before the Trump administration's &quot;zero-tolerance&quot; policy resulted in more than 2,300 children being separated from their families at the border in a mere five-week period, a ninth-grader in McAllen, Texas, was taken from his mother.
He was riding in a car with friends last spring when the car was pulled over. The teenager, brought illegally to the country by his mother as a baby, was unable to show identification. Police called immigration officials, who arrested the boy and sent him to a shelter for unaccompanied migrant children.
John Doe 2, as he is referred to in current legal filings challenging his detainment, became one of thousands caught in a network of shelters and higher-security facilities that house undocumented minors, now gaining attention as newly separated children have been streaming in.
Immigration attorneys working directly with migrant children say some of these facilities provide the best care they can, given the circumstances. And a huge shelter in Brownsville, Texas, which opened its doors for a media tour last week, appeared to be clean and well-staffed at the time.
But John Doe 2 landed in a far more troubled corner of the system, according to a first-person sworn declaration in a current legal motion against the federal government for unlawful and inappropriate detainment of children. His account is one of dozens describing overloaded and secretive shelters, treatment centers and secure detention facilities for undocumented minors, which at their worst have allegedly been home to neglect, assault and other horrific abuse.
</code></pre>

<pre><code class="language-python"># Checking what other attributes are available
print(dir(article))
</code></pre>

<pre><code>['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'authors', 'date_download', 'date_modify', 'date_publish', 'description', 'filename', 'get_dict', 'image_url', 'language', 'localpath', 'source_domain', 'text', 'title', 'title_page', 'title_rss', 'url']
</code></pre>

<pre><code class="language-python">print(article.authors)
</code></pre>

<pre><code>['Blake Ellis', 'Melanie Hicken', 'Bob Ortega', 'Cnn Investigates']
</code></pre>

<pre><code class="language-python">print(article.description)
</code></pre>

<pre><code>In court documents, children allege grave abuse at migrant detention facilities, ranging from handcuffs to assaults to drugs disguised as vitamins.
</code></pre>

<h2 id="step-2-identifying-the-sentiment-of-the-news-article">Step 2: Identifying the sentiment of the news article</h2>

<h3 id="wordtokenize-the-text-remove-stopwords">WordTokenize the text; remove stopwords</h3>

<h3 id="use-the-loughranmcdonald-word-library-https-sraf-nd-edu-textual-analysis-resources-lm-20sentiment-20word-20lists-to-identify-positive-and-negative-words-and-then-score-the-article">Use the LoughranMcDonald word library (<a href="https://sraf.nd.edu/textual-analysis/resources/#LM%20Sentiment%20Word%20Lists">https://sraf.nd.edu/textual-analysis/resources/#LM%20Sentiment%20Word%20Lists</a>) to identify positive and negative words and then score the article.</h3>

<pre><code class="language-python">import pandas as pd
from nltk.tokenize import WordPunctTokenizer as wpt
</code></pre>

<pre><code class="language-python"># Tokenize the words and convert to lowercase
words = wpt().tokenize(article.text)
words = [word.lower() for word in words]
</code></pre>

<pre><code class="language-python">print(words)
</code></pre>

<pre><code>['(', 'cnn', ')', 'a', 'year', 'before', 'the', 'trump', 'administration', &quot;'&quot;, 's', '&quot;', 'zero', '-', 'tolerance', '&quot;', 'policy', 'resulted', 'in', 'more', 'than', '2', ',', '300', 'children', 'being', 'separated', 'from', 'their', 'families', 'at', 'the', 'border', 'in', 'a', 'mere', 'five', '-', 'week', 'period', ',', 'a', 'ninth', '-', 'grader', 'in', 'mcallen', ',', 'texas', ',', 'was', 'taken', 'from', 'his', 'mother', '.', 'he', 'was', 'riding', 'in', 'a', 'car', 'with', 'friends', 'last', 'spring', 'when', 'the', 'car', 'was', 'pulled', 'over', '.', 'the', 'teenager', ',', 'brought', 'illegally', 'to', 'the', 'country', 'by', 'his', 'mother', 'as', 'a', 'baby', ',', 'was', 'unable', 'to', 'show', 'identification', '.', 'police', 'called', 'immigration', 'officials', ',', 'who', 'arrested', 'the', 'boy', 'and', 'sent', 'him', 'to', 'a', 'shelter', 'for', 'unaccompanied', 'migrant', 'children', '.', 'john', 'doe', '2', ',', 'as', 'he', 'is', 'referred', 'to', 'in', 'current', 'legal', 'filings', 'challenging', 'his', 'detainment', ',', 'became', 'one', 'of', 'thousands', 'caught', 'in', 'a', 'network', 'of', 'shelters', 'and', 'higher', '-', 'security', 'facilities', 'that', 'house', 'undocumented', 'minors', ',', 'now', 'gaining', 'attention', 'as', 'newly', 'separated', 'children', 'have', 'been', 'streaming', 'in', '.', 'immigration', 'attorneys', 'working', 'directly', 'with', 'migrant', 'children', 'say', 'some', 'of', 'these', 'facilities', 'provide', 'the', 'best', 'care', 'they', 'can', ',', 'given', 'the', 'circumstances', '.', 'and', 'a', 'huge', 'shelter', 'in', 'brownsville', ',', 'texas', ',', 'which', 'opened', 'its', 'doors', 'for', 'a', 'media', 'tour', 'last', 'week', ',', 'appeared', 'to', 'be', 'clean', 'and', 'well', '-', 'staffed', 'at', 'the', 'time', '.', 'but', 'john', 'doe', '2', 'landed', 'in', 'a', 'far', 'more', 'troubled', 'corner', 'of', 'the', 'system', ',', 'according', 'to', 'a', 'first', '-', 'person', 'sworn', 'declaration', 'in', 'a', 'current', 'legal', 'motion', 'against', 'the', 'federal', 'government', 'for', 'unlawful', 'and', 'inappropriate', 'detainment', 'of', 'children', '.', 'his', 'account', 'is', 'one', 'of', 'dozens', 'describing', 'overloaded', 'and', 'secretive', 'shelters', ',', 'treatment', 'centers', 'and', 'secure', 'detention', 'facilities', 'for', 'undocumented', 'minors', ',', 'which',  'about', 'facilities', 'holding', 'migrant', 'children', '?', 'email', 'us', ':', 'watchdog', '@', 'cnn', '.', 'com', '.']
</code></pre>

<pre><code class="language-python">#Load stopword lists
stopwords = pd.read_csv(&quot;StopWords_GenericLong.txt&quot;,names=['Word'])
stop_list = stopwords['Word'].tolist()
</code></pre>

<pre><code class="language-python">print(stop_list)
</code></pre>

<pre><code>['a', &quot;a's&quot;, 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', &quot;ain't&quot;, 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', &quot;aren't&quot;, 'around', 'as', 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'b', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'c', &quot;c'mon&quot;, &quot;c's&quot;, 'came', 'can', &quot;can't&quot;, 'cannot', 'cant', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', 'co', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', &quot;couldn't&quot;, 'course', 'currently', 'd', 'definitely', 'described', 'despite', 'did', &quot;didn't&quot;, 'different', 'do', 'does', &quot;doesn't&quot;, 'doing', &quot;don't&quot;, 'done', 'down', 'downwards', 'during', 'e', 'each', 'with', 'within', 'without', &quot;won't&quot;, 'wonder', 'would', 'would', &quot;wouldn't&quot;, 'x', 'y', 'yes', 'yet', 'you', &quot;you'd&quot;, &quot;you'll&quot;, &quot;you're&quot;, &quot;you've&quot;, 'your', 'yours', 'yourself', 'yourselves', 'z', 'zero']
</code></pre>

<pre><code class="language-python"># Remove the stopwords
words_new = [word for word in words if word not in stop_list]
</code></pre>

<pre><code class="language-python">print (words_new)
</code></pre>

<pre><code>['(', 'cnn', ')', 'year', 'trump', 'administration', &quot;'&quot;, '&quot;', '-', 'tolerance', '&quot;', 'policy', 'resulted', '2', ',', '300', 'children', 'separated', 'families', 'border', 'mere', '-', 'week', 'period', ',', 'ninth', '-', 'grader', 'mcallen', ',', 'texas', ',', 'mother', '.', 'riding', 'car', 'friends', 'spring', 'car', 'pulled', '.', 'teenager', ',', 'brought', 'illegally', 'country', 'mother', 'baby', ',', 'unable', 'show', 'identification', '.', 'police', 'called', 'immigration', 'officials', ',', 'arrested', 'boy', 'shelter', 'unaccompanied', 'migrant', 'children', '.', 'john', 'doe', '2', ',', 'referred', 'current', 'legal', 'filings', 'challenging', 'detainment', ',', 'thousands', 'caught', 'network', 'shelters', 'higher', '-', 'security', 'facilities', 'house', 'undocumented', 'minors', ',', 'gaining', 'attention', 'newly', 'separated', 'children', 'streaming', '.', 'immigration', 'attorneys', 'working', 'directly', 'migrant', 'children', 'facilities', 'provide', 'care', ',', 'circumstances', '.', 'huge', 'shelter', 'brownsville', ',', 'texas', ',', 'opened', 'doors', 'media', 'tour', 'week', ',', 'appeared', 'clean', '-', 'staffed', 'time', '.', 'john', 'doe', '2', 'landed', 'troubled', 'corner', 'system', ',', '-', 'person', 'sworn', 'declaration', 'current', 'legal', 'motion', 'federal', 'government', 'unlawful', 'inappropriate', 'detainment', 'children', '.', 'account', 'dozens', 'describing', 'overloaded', 'secretive', 'shelters', ',', 'treatment', 'ensured', 'lifelong', 'damage', 'children', '.&quot;', 'share', 'facilities', 'holding', 'migrant', 'children', '?', 'email', ':', 'watchdog', '@', 'cnn', '.', '.']
</code></pre>

<pre><code class="language-python">#Load master dictionary
master = pd.read_csv(&quot;LoughranMcDonald_MasterDictionary_2016.csv&quot;)
</code></pre>

<pre><code class="language-python">master.head()
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Word</th>
      <th>Sequence Number</th>
      <th>Word Count</th>
      <th>Word Proportion</th>
      <th>Average Proportion</th>
      <th>Std Dev</th>
      <th>Doc Count</th>
      <th>Negative</th>
      <th>Positive</th>
      <th>Uncertainty</th>
      <th>Litigious</th>
      <th>Constraining</th>
      <th>Superfluous</th>
      <th>Interesting</th>
      <th>Modal</th>
      <th>Irr_Verb</th>
      <th>Harvard_IV</th>
      <th>Syllables</th>
      <th>Source</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>AARDVARK</td>
      <td>1</td>
      <td>275</td>
      <td>1.603442e-08</td>
      <td>1.306189e-08</td>
      <td>3.665256e-06</td>
      <td>82</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>12of12inf</td>
    </tr>
    <tr>
      <th>1</th>
      <td>AARDVARKS</td>
      <td>2</td>
      <td>3</td>
      <td>1.749209e-10</td>
      <td>1.028197e-11</td>
      <td>1.014208e-08</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>12of12inf</td>
    </tr>
    <tr>
      <th>2</th>
      <td>ABACI</td>
      <td>3</td>
      <td>8</td>
      <td>4.664558e-10</td>
      <td>1.465871e-10</td>
      <td>6.401309e-08</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>12of12inf</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ABACK</td>
      <td>4</td>
      <td>6</td>
      <td>3.498419e-10</td>
      <td>1.758203e-10</td>
      <td>7.213526e-08</td>
      <td>6</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>12of12inf</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ABACUS</td>
      <td>5</td>
      <td>6729</td>
      <td>3.923477e-07</td>
      <td>3.752169e-07</td>
      <td>3.452425e-05</td>
      <td>845</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>12of12inf</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">positive = master[master[&quot;Positive&quot;]&gt;0]
negative = master[master[&quot;Negative&quot;]&gt;0]
</code></pre>

<pre><code class="language-python">pos_words = positive[&quot;Word&quot;].tolist()
neg_words = negative[&quot;Word&quot;].tolist()
</code></pre>

<pre><code class="language-python">print(pos_words)
</code></pre>

<pre><code>['ABLE', 'ABUNDANCE', 'ABUNDANT', 'ACCLAIMED', 'ACCOMPLISH', 'ACCOMPLISHED', 'ACCOMPLISHES', 'ACCOMPLISHING', 'ACCOMPLISHMENT', 'ACCOMPLISHMENTS', 'ACHIEVE', 'ACHIEVED', 'ACHIEVEMENT', 'ACHIEVEMENTS', 'ACHIEVES', 'ACHIEVING', 'ADEQUATELY', 'ADVANCEMENT', 'ADVANCEMENTS', 'ADVANCES', 'ADVANCING', 'ADVANTAGE', 'ADVANTAGED', 'ADVANTAGEOUS', 'ADVANTAGEOUSLY', 'ADVANTAGES', 'ALLIANCE', 'ALLIANCES', 'ASSURE', 'ASSURED', 'ASSURES', 'ASSURING', 'ATTAIN', 'ATTAINED', 'ATTAINING', 'ATTAINMENT', 'ATTAINMENTS', 'ATTAINS', 'ATTRACTIVE', 'ATTRACTIVENESS', 'BEAUTIFUL', 'BEAUTIFULLY', 'BENEFICIAL', 'BENEFICIALLY', 'BENEFIT', 'BENEFITED', 'BENEFITING', 'BENEFITTED', 'BENEFITTING', 'BEST', 'BETTER', 'BOLSTERED', 'BOLSTERING', 'BOLSTERS', 'BOOM', 'BOOMING', 'BOOST', 'BOOSTED', 'BREAKTHROUGH', 'BREAKTHROUGHS', 'BRILLIANT', 'CHARITABLE', 'COLLABORATE', 'COLLABORATED', 'COLLABORATES', 'COLLABORATING', 'COLLABORATION', 'COLLABORATIONS', 'COLLABORATIVE', 'COLLABORATOR', 'COLLABORATORS', 'COMPLIMENT', 'COMPLIMENTARY', 'COMPLIMENTED', 'COMPLIMENTING', 'COMPLIMENTS', 'CONCLUSIVE', 'CONCLUSIVELY', 'CONDUCIVE', 'CONFIDENT', 'CONSTRUCTIVE', 'CONSTRUCTIVELY', 'COURTEOUS', 'CREATIVE', 'CREATIVELY', 'CREATIVENESS', 'CREATIVITY', 'DELIGHT', 'DELIGHTED', 'DELIGHTFUL', 'DELIGHTFULLY', 'DELIGHTING', 'DELIGHTS', 'DEPENDABILITY', 'DEPENDABLE', 'DESIRABLE', 'DESIRED', 'DESPITE', 'DESTINED', 'DILIGENT', 'DILIGENTLY', 'DISTINCTION', 'DISTINCTIONS', 'DISTINCTIVE', 'DISTINCTIVELY', 'DISTINCTIVENESS', 'DREAM', 'EASIER', 'EASILY', 'EASY', 'EFFECTIVE', 'EFFICIENCIES', 'EFFICIENCY', 'UNSURPASSED', 'UPTURN', 'UPTURNS', 'VALUABLE', 'VERSATILE', 'VERSATILITY', 'VIBRANCY', 'VIBRANT', 'WIN', 'WINNER', 'WINNERS', 'WINNING', 'WORTHY']
</code></pre>

<pre><code class="language-python">print(neg_words)
</code></pre>

<pre><code>['ABANDON', 'ABANDONED', 'ABANDONING', 'ABANDONMENT', 'ABANDONMENTS', 'ABANDONS', 'ABDICATED', 'ABDICATES', 'ABDICATING', 'ABDICATION', 'ABDICATIONS', 'ABERRANT', 'ABERRATION', 'ABERRATIONAL', 'ABERRATIONS', 'ABETTING', 'ABNORMAL', 'ABNORMALITIES', 'ABNORMALITY', 'ABNORMALLY', 'ABOLISH', 'ABOLISHED', 'ABOLISHES', 'ABOLISHING', 'ABROGATE', 'ABROGATED', 'ABROGATES', 'ABROGATING', 'ABROGATION', 'ABROGATIONS', 'ABRUPT', 'ABRUPTLY', 'ABRUPTNESS', 'ABSENCE', 'ABSENCES', 'ABSENTEEISM', 'ABUSE', 'ABUSED', 'ABUSES', 'ABUSING', 'ABUSIVE', 'ABUSIVELY', 'ABUSIVENESS', 'ACCIDENT', 'ACCIDENTAL', 'ACCIDENTALLY', 'ACCIDENTS', 'ACCUSATION', 'ACCUSATIONS', 'ACCUSE', 'ACCUSED', 'ACCUSES', 'ACCUSING', 'ACQUIESCE', 'ACQUIESCED', 'ACQUIESCES', 'ACQUIESCING', 'ACQUIT', 'ACQUITS', 'ACQUITTAL', 'ACQUITTALS', 'ACQUITTED', 'ACQUITTING', 'ADULTERATE', 'ADULTERATED', 'ADULTERATING', 'ADULTERATION', 'ADULTERATIONS', 'ADVERSARIAL', 'ADVERSARIES', 'ADVERSARY', 'ADVERSE', 'ADVERSELY', 'ADVERSITIES', 'ADVERSITY', 'AFTERMATH', 'AFTERMATHS', 'AGAINST', 'AGGRAVATE', 'AGGRAVATED', 'AGGRAVATES', 'AGGRAVATING', 'AGGRAVATION', 'AGGRAVATIONS', 'ALERTED', 'ALERTING', 'ALIENATE', 'ALIENATED', 'ALIENATES', 'ALIENATING', 'ALIENATION', 'ALIENATIONS', 'ALLEGATION', 'ALLEGATIONS', 'ALLEGE', 'ALLEGED', 'ALLEGEDLY', 'ALLEGES', 'ALLEGING', 'ANNOY', 'ANNOYANCE', 'ANNOYANCES', 'ANNOYED', 'ANNOYING', 'ANNOYS', 'ANNUL', 'ANNULLED', 'ANNULLING', 'ANNULMENT', 'ANNULMENTS', 'ANNULS', 'ANOMALIES', 'ANOMALOUS', 'ANOMALOUSLY', 'ANOMALY', 'ANTICOMPETITIVE', 'ANTITRUST', 'ARGUE', 'ARGUED', 'ARGUING', 'ARGUMENT', 'ARGUMENTATIVE', 'ARGUMENTS', 'ARREARAGE', 'ARREARAGES', 'ARREARS', 'ARREST', 'ARRESTED', 'ARRESTS', 'ARTIFICIALLY', 'ASSAULT', 'ASSAULTED', 'ASSAULTING', 'ASSAULTS', 'ASSERTIONS', 'ATTRITION', 'AVERSELY', 'BACKDATING', 'BAD', 'BAIL', 'BAILOUT', 'BALK', 'BALKED', 'BANKRUPT', 'BANKRUPTCIES', 'BANKRUPTCY', 'BANKRUPTED', 'BANKRUPTING', 'BANKRUPTS', 'BANS', 'BARRED', 'BARRIER', 'BARRIERS', 'BOTTLENECK', 'BOTTLENECKS', 'BOYCOTT', 'BOYCOTTED', 'BOYCOTTING', 'WEAKNESSES', 'WILLFULLY', 'WORRIES', 'WORRY', 'WORRYING', 'WORSE', 'WORSEN', 'WORSENED', 'WORSENING', 'WORSENS', 'WORST', 'WORTHLESS', 'WRITEDOWN', 'WRITEDOWNS', 'WRITEOFF', 'WRITEOFFS', 'WRONG', 'WRONGDOING', 'WRONGDOINGS', 'WRONGFUL', 'WRONGFULLY', 'WRONGLY']
</code></pre>

<pre><code class="language-python">pos_words = [word.lower() for word in pos_words]
neg_words = [word.lower() for word in neg_words]
</code></pre>

<pre><code class="language-python">words_new_pos = [word for word in words_new if word in pos_words]
words_new_neg = [word for word in words_new if word in neg_words]
</code></pre>

<pre><code class="language-python">print(&quot;Positive Score: &quot;,len(words_new_pos),&quot;Negative Score: &quot;,len(words_new_neg))
</code></pre>

<pre><code>Positive Score:  7 Negative Score:  220
</code></pre>

<pre><code class="language-python">## The above scores indicate that the given news article has a negative sentiment
</code></pre>

	</article>
</main>

<div id="bottom-nav" class="text-center center-block">
	<a href=" https://joecoelhosj.github.io/" class="btn btn-default"><i class="fa fa-home"></i> Home</a>
</div>


  <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "joecoelhosj-github-io" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


						</div>
					</div>
				</div>
			</div>
		</div>
  </div>
  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', '', 'auto');
    ga('send', 'pageview');
    window.baseURL = "https:\/\/joecoelhosj.github.io\/";
  </script>
  
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.2/js/bootstrap.min.js"></script>
  
  
  <script src="https://joecoelhosj.github.io//js/App.js"></script>
  
</body>
</html>
